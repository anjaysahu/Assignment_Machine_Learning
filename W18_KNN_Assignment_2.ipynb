{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance\n",
        "metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?"
      ],
      "metadata": {
        "id": "k3q6H8RoaKV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Difference:\n",
        "\n",
        "- Euclidean Distance: Measures the straight-line distance between two points. It's like the shortest path between two locations on a map.\n",
        "- Manhattan Distance: Measures the distance along axes, like walking on a grid with only vertical and horizontal movements. It's like navigating a city with only streets that run north-south and east-west.\n",
        "\n",
        "### Impact on KNN Performance:\n",
        "- **Sensitivity to Outliers:**\n",
        " - Euclidean: More sensitive to outliers due to the squared differences in its calculation. Outliers can significantly influence the overall distance.\n",
        " - Manhattan: Less sensitive to outliers as it only considers the absolute differences.\n",
        "- **Feature Scaling:**\n",
        " - Euclidean: More sensitive to differences in feature scales. Features with larger scales can dominate the distance calculation.\n",
        " - Manhattan: Less sensitive to feature scaling as it only considers the absolute differences.\n"
      ],
      "metadata": {
        "id": "o8DTA1nmaKS9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be\n",
        "used to determine the optimal k value?"
      ],
      "metadata": {
        "id": "sP466LP2aKQf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choosing the Optimal k:\n",
        "The value of k significantly impacts the KNN model's performance. A small k can lead to high variance (overfitting), while a large k can result in high bias (underfitting).\n",
        "\n",
        "### Techniques for Determining Optimal k:\n",
        "- Train the KNN model on a subset of folds(Cross-validation) and evaluate its performance on the remaining fold for different values of K.\n",
        "- Repeat this process for all folds and choose the K that gives the best average performance.\n",
        "- Define a range of possible K values.\n",
        "- Train(Grid search) and evaluate the KNN model for each K value in the range."
      ],
      "metadata": {
        "id": "6BcY9DPJaKNx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In\n",
        "what situations might you choose one distance metric over the other?"
      ],
      "metadata": {
        "id": "4b_PaMx4aKLC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The choice of distance metric significantly influences the performance of a KNN algorithm. Here's a breakdown of how different metrics impact KNN:\n",
        "\n",
        "**1. Euclidean Distance:**\n",
        "- Strengths:\n",
        " - Simple and intuitive to understand.\n",
        " - Well-suited for continuous numerical data where straight-line distances are meaningful.\n",
        "- Weaknesses:\n",
        " - Sensitive to outliers due to the squared differences.\n",
        " - Can be heavily influenced by features with larger scales.\n",
        "\n",
        "**2. Manhattan Distance:**\n",
        "- Strengths:\n",
        " - Less sensitive to outliers compared to Euclidean distance.\n",
        " - Less sensitive to differences in feature scales.\n",
        " - More suitable for data where movements are restricted to specific directions (e.g., city grids) or when feature scales vary significantly.\n",
        "- Weaknesses:\n",
        " - May not accurately capture the true distance between points in some cases.\n",
        "\n",
        "**Choosing the Right Metric:**\n",
        "- Euclidean: Ideal for data with continuous numerical features and minimal outliers.\n",
        "- Manhattan: Preferable when dealing with data that has:\n",
        " - Significant outliers.\n",
        " - Features with vastly different scales.\n",
        " - Data where movements are restricted to specific directions."
      ],
      "metadata": {
        "id": "TfezfkFVaKIi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect\n",
        "the performance of the model? How might you go about tuning these hyperparameters to improve\n",
        "model performance?"
      ],
      "metadata": {
        "id": "FIVhxNiLaKFz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Common Hyperparameters in KNN:\n",
        "- Number of Neighbors (k):\n",
        " - Impact: Directly influences the model's bias-variance trade-off. A small k can lead to high variance (overfitting), while a large k can result in high bias (underfitting).\n",
        "- Distance Metric:\n",
        " - Impact: As discussed earlier, the choice of distance metric (Euclidean, Manhattan, etc.) significantly affects how distances between data points are calculated, impacting model performance.\n",
        "\n",
        "### Hyperparameter Tuning Techniques:\n",
        "- Grid Search:\n",
        " - Define a range of values for each hyperparameter.\n",
        " - Train and evaluate the KNN model for all possible combinations of hyperparameter values within the specified ranges.\n",
        " - Select the combination that yields the best performance.\n",
        "\n",
        "- Random Search:\n",
        " - Randomly sample hyperparameter values from their respective ranges.\n",
        " - This approach can be more efficient than grid search, especially for high-dimensional hyperparameter spaces."
      ],
      "metadata": {
        "id": "hM5VbWAvaKDG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What\n",
        "techniques can be used to optimize the size of the training set?"
      ],
      "metadata": {
        "id": "GNI7dxzSaKAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Impact of Training Set Size:\n",
        "\n",
        "- Small Training Set:\n",
        " - High Variance: The model may overfit to the training data, performing poorly on unseen data.\n",
        " - Poor Generalization: The model may not accurately capture the underlying patterns in the data, leading to poor generalization performance.\n",
        "\n",
        "- Large Training Set:\n",
        " - Reduced Variance: With more data, the model can better capture the underlying data distribution, leading to improved generalization.\n",
        " - Increased Computational Cost: Larger datasets require more memory and computational resources for distance calculations.\n",
        "\n",
        " ### Optimizing Training Set Size:\n",
        "\n",
        "- Data Collection:\n",
        " - Active Learning: Strategically select and label new data points that are most informative for the model.\n",
        " - Data Augmentation: Create synthetic data points by applying transformations (e.g., rotations, flips) to existing data.\n",
        "- Data Selection:\n",
        " - Oversampling: Increase the representation of minority classes in imbalanced datasets.\n",
        " - Undersampling: Reduce the number of instances in the majority class."
      ],
      "metadata": {
        "id": "ocH81bHMaJ9f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you\n",
        "overcome these drawbacks to improve the performance of the model?"
      ],
      "metadata": {
        "id": "m0kYIOR-aJ6b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Drawbacks of KNN:\n",
        "\n",
        "- Computational Cost: Calculating distances to all training points can be computationally expensive, especially for large datasets or high-dimensional data.\n",
        "- Sensitivity to Noise: Noise in the training data can significantly impact the performance, especially with small values of k.\n",
        "- Choice of k: Selecting the optimal value of k can be crucial and may require careful tuning.\n",
        "\n",
        "### Overcoming Drawbacks:\n",
        "- Approximation Techniques: Use approximate nearest neighbor search algorithms (e.g., k-d trees, ball trees) to speed up distance calculations.\n",
        "- Data Cleaning: Remove noise and outliers from the training data.\n",
        "- Hyperparameter Tuning: Use techniques like cross-validation and grid search to find the optimal value of k and other hyperparameters."
      ],
      "metadata": {
        "id": "m4BFain9aJ2x"
      }
    }
  ]
}