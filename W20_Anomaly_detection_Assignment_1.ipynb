{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is anomaly detection and what is its purpose?\n"
      ],
      "metadata": {
        "id": "K0PfHohxRvVp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anomaly detection, also known as outlier detection, is a technique used to identify data points, events, or observations that deviate significantly from a normal pattern or expected behavior. These anomalies can be indicative of potential issues, errors, or even malicious activities.\n",
        "\n",
        "**Purpose of Anomaly Detection:**\n",
        "Anomaly detection serves a variety of purposes across different industries:  \n",
        "- Fraud Detection: Identifying unusual financial transactions or patterns that may signal fraudulent activity.\n",
        "- Network Security: Detecting malicious network traffic or intrusions by identifying abnormal network behavior."
      ],
      "metadata": {
        "id": "Udqfec-6RvSw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What are the key challenges in anomaly detection?"
      ],
      "metadata": {
        "id": "WPj9RMExRvO9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anomaly detection can be a complex task, and several key challenges often arise:\n",
        "- Handling Imbalanced Data: Anomaly detection often deals with imbalanced datasets, where normal data points significantly outnumber anomalies. This can lead to biased models that struggle to identify rare but important anomalies.\n",
        "- High-Dimensional Data: Many real-world datasets have numerous features, which can make it challenging to identify meaningful patterns and anomalies.\n",
        "- Noise and Outliers: Noise and outliers in the data can interfere with anomaly detection algorithms, leading to false positives or false negatives.\n",
        "- Computational Complexity: Some anomaly detection techniques can be computationally expensive, especially when dealing with large datasets."
      ],
      "metadata": {
        "id": "ety3-UCtRvNq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?\n"
      ],
      "metadata": {
        "id": "grxRvGHWRvLO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The primary difference between unsupervised and supervised anomaly detection lies in the availability of labeled data:\n",
        "\n",
        "**Unsupervised Anomaly Detection:**      \n",
        "- No labeled data: This approach assumes that most of the data is normal and aims to identify outliers without prior knowledge of what constitutes an anomaly.\n",
        "- Statistical methods and clustering: Common techniques include statistical methods (e.g., Z-score, outlier detection using interquartile range) and clustering algorithms.\n",
        "\n",
        "**Supervised Anomaly Detection:**    \n",
        "- Labeled data: This approach requires a dataset with labeled examples of both normal and anomalous data points.\n",
        "- Classification algorithms: Common techniques include classification algorithms (e.g., decision trees, random forests, support vector machines) and neural networks."
      ],
      "metadata": {
        "id": "MlRTLhCiRvJd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What are the main categories of anomaly detection algorithms?"
      ],
      "metadata": {
        "id": "lI8FzfJ1RvE-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anomaly detection algorithms can be broadly categorized into the following:\n",
        "- Statistical Methods\n",
        "- Clustering-Based Methods\n",
        "- Machine Learning-Based Methods"
      ],
      "metadata": {
        "id": "GWEgdEy_RvD1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. What are the main assumptions made by distance-based anomaly detection methods?\n"
      ],
      "metadata": {
        "id": "1nyz-ESLRu_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distance-based anomaly detection methods, such as Local Outlier Factor (LOF), assume that:   \n",
        "- Normal data points are clustered together: This means that normal data points are closely related to their neighbors in the feature space.\n",
        "- Anomalies are isolated: Anomalies are data points that are significantly distant from their nearest neighbors compared to other data points.\n",
        "- Distance metric is meaningful: The chosen distance metric (e.g., Euclidean distance, Manhattan distance) accurately reflects the similarity or dissimilarity between data points."
      ],
      "metadata": {
        "id": "0oLYhPjwRu9j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. How does the LOF algorithm compute anomaly scores?"
      ],
      "metadata": {
        "id": "mxeKplgETIGC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LOF algorithm calculates anomaly scores based on the local density deviation of a data point compared to its neighbors. Here's a step-by-step breakdown:\n",
        "- k-Nearest Neighbors (kNN): For each data point, the algorithm identifies its k nearest neighbors.\n",
        "- Reachability Distance: The reachability distance between two points, p and q, is defined as the maximum of the distance between p and q and the distance between q and its k-nearest neighbor.\n",
        "- Local Reachability Density (LRD): The LRD of a point p is calculated as the inverse of the average reachability distance of p to its k-nearest neighbors.\n",
        "- Local Outlier Factor (LOF): The LOF score of a point p is the average ratio of the LRD of p to the LRD of its k-nearest neighbors."
      ],
      "metadata": {
        "id": "yhgRwsFTTNI-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. What are the key parameters of the Isolation Forest algorithm?\n"
      ],
      "metadata": {
        "id": "pShFp0LyTNF-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Isolation Forest algorithm has two key parameters:\n",
        "\n",
        "- Number of Trees (num_trees): This parameter controls the number of decision trees that are constructed in the forest. A higher number of trees generally improves the accuracy of the algorithm, but it also increases the computational cost.\n",
        "- Subsample Size (max_samples): This parameter determines the number of samples drawn from the original dataset to train each decision tree. A smaller subsample size can lead to faster training times, but it may also reduce the accuracy of the algorithm."
      ],
      "metadata": {
        "id": "OJsx7MulTNCl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score\n",
        "using KNN with K=10?\n"
      ],
      "metadata": {
        "id": "ANGMvzHtTNAT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In KNN with K=10, we consider the 10 nearest neighbors of a data point. Since the data point has only 2 neighbors of the same class within a radius of 0.5, the remaining 8 neighbors must belong to different classes. This indicates that the data point is likely an outlier or anomaly.\n",
        "\n",
        "However, to calculate a precise anomaly score, we would need more information about the distance metric used, the distribution of the data, and the specific KNN algorithm implementation.\n",
        "\n"
      ],
      "metadata": {
        "id": "CyccHnO3TM-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the\n",
        "anomaly score for a data point that has an average path length of 5.0 compared to the average path\n",
        "length of the trees?"
      ],
      "metadata": {
        "id": "mOwrLlHfTM7m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Isolation Forest, anomaly scores are typically calculated based on the average path length of a data point compared to the average path length of the trees. A shorter average path length indicates a higher probability of the data point being an anomaly.\n",
        "\n",
        "To calculate the anomaly score in this case, we would need to know the average path length of the trees in the forest.\n",
        "\n",
        "However, we can make an inference:    \n",
        "- If the average path length of the trees is significantly higher than 5.0, then the data point with an average path length of 5.0 is likely to have a low anomaly score, indicating it's not an outlier.\n",
        "- If the average path length of the trees is significantly lower than 5.0, then the data point with an average path length of 5.0 is likely to have a high anomaly score, indicating it's an outlier."
      ],
      "metadata": {
        "id": "p5j0YPNaTM5d"
      }
    }
  ]
}