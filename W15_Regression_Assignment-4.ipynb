{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1094a2e-efe0-41b1-84c3-595f6c9d62d1",
   "metadata": {},
   "source": [
    "Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac30d7c-639c-41ad-8acb-607f08a6d149",
   "metadata": {},
   "source": [
    "#### Answer -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72a1d9a-3a8a-40ef-b875-71e73f637c04",
   "metadata": {},
   "source": [
    "Lasso Regression, which stands for Least Absolute Shrinkage and Selection Operator, is a type of regression analysis that incorporates a regularization technique to prevent overfitting and improve model performance. It's particularly useful when dealing with datasets that have a large number of features.   \n",
    "\n",
    "**Key Differences from Other Regression Techniques:**   \n",
    "**Regularization:** Unlike traditional linear regression, Lasso incorporates a regularization term to prevent overfitting and improve model generalization.      \n",
    "**Feature Selection:** Lasso has the ability to automatically select relevant features by setting the coefficients of irrelevant features to zero. This leads to simpler and more interpretable models.       \n",
    "**Penalty Term:** Lasso uses the L1 norm as a penalty term, while Ridge regression (another regularization technique) uses the L2 norm. This difference in penalty terms leads to different behaviors in terms of coefficient shrinkage and feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9c232d-457b-4e0c-8824-3efb44b68a9d",
   "metadata": {},
   "source": [
    "Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea5b3d6-111b-4245-a341-86231cd8e402",
   "metadata": {},
   "source": [
    "#### Answer -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd34f46-78ad-4a90-87b9-e50a537453c9",
   "metadata": {},
   "source": [
    "Lasso Regression's primary advantage in feature selection is its ability to automatically perform variable selection by shrinking the coefficients of irrelevant features to exactly zero.   \n",
    "\n",
    "This means that Lasso not only improves model performance by preventing overfitting but also provides a built-in mechanism to identify the most important features in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb55e5f-0dc4-4bf5-9a49-b42d43346733",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136f96de-34df-4929-8486-d2eabf7143db",
   "metadata": {},
   "source": [
    "#### Answer -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd80e3a3-fe7b-4da0-a81c-60e5bc6287f0",
   "metadata": {},
   "source": [
    "**Non-zero Coefficients:** Coefficients that are non-zero represent features that the model considers important in predicting the target variable. The interpretation of these coefficients is identical to linear regression: a one-unit increase in the feature is associated with a change of the coefficient value in the target variable, holding all other features constant.           \n",
    "**Zero Coefficients:** Features with zero coefficients are considered irrelevant by the model and have been effectively removed from the equation. This is a significant advantage of Lasso Regression over other regression techniques.           \n",
    "**Magnitude of Coefficients:** While the sign of the coefficient indicates the direction of the relationship (positive or negative), the magnitude of non-zero coefficients can be less reliable for comparison purposes compared to ordinary least squares due to the shrinkage effect of Lasso.          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ac6ae8-99f7-4d06-9ba3-8cde3d014da1",
   "metadata": {},
   "source": [
    "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the\n",
    "model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4c8253-d3c5-447f-832e-44478d52e637",
   "metadata": {},
   "source": [
    "#### Answer -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7d694b-aa33-4e58-9e40-d7e397b05579",
   "metadata": {},
   "source": [
    "The primary tuning parameter in Lasso Regression is:  \n",
    "\n",
    "**Lambda(λ)**                \n",
    "- **Purpose:** Controls the strength of the L1 regularization penalty.      \n",
    "- **Impact:**                                                             \n",
    "  - **Low lambda:** Less shrinkage, model closer to ordinary least squares.        \n",
    "  - **High lambda:** More shrinkage, more features driven to zero, simpler model.        \n",
    "      \n",
    "**Impact on Model Performance**                 \n",
    "- **Underfitting:** Too high an lambda can lead to underfitting by excluding important features.            \n",
    "- **Overfitting:** Too low an lambda might result in overfitting by including too many features.            \n",
    "- **Optimal lambda:** Balances bias and variance, leading to the best predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c94a96-655a-4b4b-bcb3-20bd374c3844",
   "metadata": {},
   "source": [
    "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bf424f-0860-40b6-a717-965354713810",
   "metadata": {},
   "source": [
    "#### Answer -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eccca4f-17df-4bb1-bbdb-5d3f708c1aa5",
   "metadata": {},
   "source": [
    "Lasso Regression is inherently a linear model. It assumes a linear relationship between the independent variables and the dependent variable. However, we can extend its capabilities to handle non-linear relationships through a few approaches:                       \n",
    "\n",
    "**1. Feature Engineering:**                     \n",
    "- **Polynomial Features:** Create new features by squaring, cubing, or raising original features to higher powers. This can capture non-linear patterns.      \n",
    "- **Interaction Terms:** Combine existing features to create new ones, potentially capturing complex interactions.                \n",
    "- **Transformations:** Apply transformations like log, square root, or exponential to variables to induce non-linearity. \n",
    "\n",
    "**2. Basis Expansions:**                  \n",
    "- **Splines:** Divide the range of a variable into intervals and fit piecewise polynomial functions. This allows for flexible modeling of non-linear relationships.              \n",
    "- **Radial Basis Functions (RBFs):** Use functions that decrease with distance from a center to capture non-linear patterns.    \n",
    "\n",
    "**3. Kernel Methods:**                    \n",
    "- **Kernel Ridge Regression:** While not Lasso specifically, it's related and can handle non-linearity by implicitly mapping data to a higher-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e9c2a9-05af-4174-b875-bca3f75bae5e",
   "metadata": {},
   "source": [
    "Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a7a7f1-a7c8-48e7-94ee-e6586de7447f",
   "metadata": {},
   "source": [
    "#### Answer -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66c58b4-0a32-4444-a064-4fe85c4e89a9",
   "metadata": {},
   "source": [
    "Both Ridge and Lasso regression are regularization techniques used to prevent overfitting in linear regression models. However, they differ in how they penalize the model coefficients.               \n",
    "                                    \n",
    "**Ridge Regression**          \n",
    "- **L2 regularization:** Adds the sum of the squares of the coefficients to the loss function.            \n",
    "- **Shrinks coefficients towards zero:** Reduces the magnitude of coefficients but doesn't eliminate any.             \n",
    "- **Effective for:** Multicollinearity (correlated features) and improving model generalization.   \n",
    "\n",
    "**Lasso Regression**          \n",
    "- **L1 regularization:** Adds the sum of the absolute values of the coefficients to the loss function.                         \n",
    "- **Feature selection:** Can force some coefficients to be exactly zero, effectively performing feature selection.        \n",
    "- **Effective for:** High-dimensional datasets, feature selection, and building simpler models.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4e83c5-d7f6-4d32-a596-120afdc27444",
   "metadata": {},
   "source": [
    "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc340f94-988a-431e-8f52-7f4a5e742347",
   "metadata": {},
   "source": [
    "#### Answer -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d98273-0c37-45b8-a46d-1fbcaa62d0e7",
   "metadata": {},
   "source": [
    "Yes, Lasso Regression can handle multicollinearity effectively.   \n",
    "\n",
    "**How Lasso Handles Multicollinearity**              \n",
    "Lasso achieves this through its L1 regularization. Here's how:   \n",
    "\n",
    "- **Shrinking Coefficients:** In the presence of highly correlated features, Lasso tends to shrink the coefficients of these features towards zero.         \n",
    "- **Feature Selection:** Often, Lasso will select one of the highly correlated features and set the coefficients of the others to zero. This helps to reduce redundancy and improve model interpretability.             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb99e37-247f-446b-9d0c-60fd82e05a42",
   "metadata": {},
   "source": [
    "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e40e702-326d-4d4d-b533-0e035925a333",
   "metadata": {},
   "source": [
    "#### Answer -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6138545-095a-4631-a18a-de6e90a127d5",
   "metadata": {},
   "source": [
    "The optimal value of the regularization parameter (lambda) in Lasso Regression significantly impacts the model's performance. A crucial step in building a robust Lasso model is to determine the best lambda value.        \n",
    "\n",
    "**Cross-Validation**         \n",
    "The most common and effective method to select the optimal lambda is cross-validation. This involves:   \n",
    "\n",
    "**1. Splitting the data:** Divide the dataset into multiple folds (commonly 5 or 10).                 \n",
    "**2. Training and evaluation:** For each lambda value in a predefined range:\n",
    "- Train the Lasso model on a subset of folds.\n",
    "- Evaluate the model's performance on the remaining fold.\n",
    "\n",
    "**Common Cross-Validation Techniques**         \n",
    "- **K-fold cross-validation:** The dataset is divided into K equal-sized folds.   \n",
    "- **Leave-one-out cross-validation (LOOCV):** Each observation is used as a validation set once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1eaebe-fca5-4dcc-a83a-09d82578f42e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
